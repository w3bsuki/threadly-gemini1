name: Database Backup

on:
  schedule:
    # Run daily at 2 AM UTC
    - cron: '0 2 * * *'
  workflow_dispatch: # Allow manual trigger

env:
  NODE_VERSION: '20'
  PNPM_VERSION: '8'

jobs:
  backup-production:
    name: Backup Production Database
    runs-on: ubuntu-latest
    environment: production
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Install PostgreSQL client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client

      - name: Create database backup
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL_PROD }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}
          S3_BACKUP_BUCKET: ${{ secrets.S3_BACKUP_BUCKET }}
        run: |
          # Extract database connection details
          DB_HOST=$(echo $DATABASE_URL | sed -n 's/.*@\([^:]*\):.*/\1/p')
          DB_PORT=$(echo $DATABASE_URL | sed -n 's/.*:\([0-9]*\)\/.*/\1/p')
          DB_NAME=$(echo $DATABASE_URL | sed -n 's/.*\/\([^?]*\).*/\1/p')
          DB_USER=$(echo $DATABASE_URL | sed -n 's/.*:\/\/\([^:]*\):.*/\1/p')
          DB_PASSWORD=$(echo $DATABASE_URL | sed -n 's/.*:\/\/[^:]*:\([^@]*\)@.*/\1/p')
          
          # Create backup filename with timestamp
          BACKUP_FILE="threadly-backup-$(date +%Y%m%d-%H%M%S).sql"
          
          # Create database dump
          PGPASSWORD=$DB_PASSWORD pg_dump \
            -h $DB_HOST \
            -p $DB_PORT \
            -U $DB_USER \
            -d $DB_NAME \
            --no-password \
            --verbose \
            --clean \
            --if-exists \
            --create \
            --format=custom \
            --file=$BACKUP_FILE
          
          # Compress backup
          gzip $BACKUP_FILE
          
          # Upload to S3
          aws s3 cp ${BACKUP_FILE}.gz s3://${S3_BACKUP_BUCKET}/database/production/
          
          # Clean up local file
          rm ${BACKUP_FILE}.gz
          
          echo "Backup completed: ${BACKUP_FILE}.gz"

      - name: Verify backup integrity
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL_PROD }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}
          S3_BACKUP_BUCKET: ${{ secrets.S3_BACKUP_BUCKET }}
        run: |
          # Get latest backup file
          LATEST_BACKUP=$(aws s3 ls s3://${S3_BACKUP_BUCKET}/database/production/ --recursive | sort | tail -n 1 | awk '{print $4}')
          
          # Download and verify backup
          aws s3 cp s3://${S3_BACKUP_BUCKET}/${LATEST_BACKUP} ./verify-backup.sql.gz
          gunzip verify-backup.sql.gz
          
          # Check if backup file is valid
          if pg_restore --list verify-backup.sql > /dev/null 2>&1; then
            echo "‚úÖ Backup verification successful"
          else
            echo "‚ùå Backup verification failed"
            exit 1
          fi
          
          # Clean up verification file
          rm verify-backup.sql

      - name: Cleanup old backups
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}
          S3_BACKUP_BUCKET: ${{ secrets.S3_BACKUP_BUCKET }}
        run: |
          # Keep backups for 30 days, delete older ones
          aws s3 ls s3://${S3_BACKUP_BUCKET}/database/production/ | while read -r line; do
            DATE=$(echo $line | awk '{print $1" "$2}')
            FILE=$(echo $line | awk '{print $4}')
            
            if [[ "$OSTYPE" == "darwin"* ]]; then
              FILE_DATE=$(date -j -f "%Y-%m-%d %H:%M:%S" "$DATE" "+%s")
            else
              FILE_DATE=$(date -d "$DATE" "+%s")
            fi
            
            CURRENT_DATE=$(date "+%s")
            DAYS_OLD=$(( (CURRENT_DATE - FILE_DATE) / 86400 ))
            
            if [ $DAYS_OLD -gt 30 ]; then
              echo "Deleting old backup: $FILE (${DAYS_OLD} days old)"
              aws s3 rm s3://${S3_BACKUP_BUCKET}/database/production/$FILE
            fi
          done

      - name: Send notification on failure
        if: failure()
        uses: 8398a7/action-slack@v3
        with:
          status: failure
          webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
          text: |
            üö® **Database Backup Failed**
            
            The scheduled database backup for production has failed.
            Please check the logs and ensure the backup system is working properly.
            
            **Action Required:** Investigate backup failure immediately.

  test-restore:
    name: Test Database Restore
    runs-on: ubuntu-latest
    needs: backup-production
    environment: staging
    services:
      postgres:
        image: postgres:15
        env:
          POSTGRES_DB: threadly_test_restore
          POSTGRES_USER: postgres
          POSTGRES_PASSWORD: postgres
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
        ports:
          - 5432:5432
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install PostgreSQL client
        run: |
          sudo apt-get update
          sudo apt-get install -y postgresql-client

      - name: Test restore process
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}
          S3_BACKUP_BUCKET: ${{ secrets.S3_BACKUP_BUCKET }}
          TEST_DATABASE_URL: postgresql://postgres:postgres@localhost:5432/threadly_test_restore
        run: |
          # Get latest backup
          LATEST_BACKUP=$(aws s3 ls s3://${S3_BACKUP_BUCKET}/database/production/ --recursive | sort | tail -n 1 | awk '{print $4}')
          
          # Download backup
          aws s3 cp s3://${S3_BACKUP_BUCKET}/${LATEST_BACKUP} ./test-restore.sql.gz
          gunzip test-restore.sql.gz
          
          # Restore to test database
          pg_restore \
            --dbname=$TEST_DATABASE_URL \
            --verbose \
            --clean \
            --if-exists \
            --no-owner \
            --no-privileges \
            test-restore.sql
          
          # Verify tables exist
          psql $TEST_DATABASE_URL -c "\dt" | grep -E "(User|Product|Order|Message)" || {
            echo "‚ùå Restore verification failed - core tables missing"
            exit 1
          }
          
          echo "‚úÖ Database restore test successful"
          
          # Clean up test file
          rm test-restore.sql

  backup-analytics:
    name: Backup Analytics Data
    runs-on: ubuntu-latest
    environment: production
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Node.js
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: Setup pnpm
        uses: pnpm/action-setup@v2
        with:
          version: ${{ env.PNPM_VERSION }}

      - name: Install dependencies
        run: pnpm install --frozen-lockfile

      - name: Export analytics data
        env:
          DATABASE_URL: ${{ secrets.DATABASE_URL_PROD }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_DEFAULT_REGION: ${{ secrets.AWS_DEFAULT_REGION }}
          S3_BACKUP_BUCKET: ${{ secrets.S3_BACKUP_BUCKET }}
        run: |
          # Create analytics export script
          cat > export-analytics.sql << 'EOF'
          \copy (
            SELECT 
              p.id,
              p.title,
              p.price,
              p.condition,
              p.status,
              p.views,
              p.favorites,
              p.created_at,
              c.name as category,
              u.location as seller_location
            FROM products p
            JOIN categories c ON p.category_id = c.id
            JOIN users u ON p.seller_id = u.id
            WHERE p.created_at >= CURRENT_DATE - INTERVAL '7 days'
          ) TO '/tmp/weekly_products.csv' WITH CSV HEADER;
          
          \copy (
            SELECT 
              o.id,
              o.amount,
              o.status,
              o.created_at,
              DATE_PART('hour', o.created_at) as hour_of_day,
              DATE_PART('dow', o.created_at) as day_of_week
            FROM orders o
            WHERE o.created_at >= CURRENT_DATE - INTERVAL '7 days'
          ) TO '/tmp/weekly_orders.csv' WITH CSV HEADER;
          
          \copy (
            SELECT 
              DATE(created_at) as date,
              COUNT(*) as daily_signups
            FROM users
            WHERE created_at >= CURRENT_DATE - INTERVAL '30 days'
            GROUP BY DATE(created_at)
            ORDER BY date
          ) TO '/tmp/daily_signups.csv' WITH CSV HEADER;
          EOF
          
          # Run analytics export
          psql $DATABASE_URL -f export-analytics.sql
          
          # Create timestamped directory
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          ANALYTICS_DIR="analytics-${TIMESTAMP}"
          mkdir $ANALYTICS_DIR
          
          # Move files to timestamped directory
          mv /tmp/weekly_products.csv $ANALYTICS_DIR/
          mv /tmp/weekly_orders.csv $ANALYTICS_DIR/
          mv /tmp/daily_signups.csv $ANALYTICS_DIR/
          
          # Create archive
          tar -czf ${ANALYTICS_DIR}.tar.gz $ANALYTICS_DIR
          
          # Upload to S3
          aws s3 cp ${ANALYTICS_DIR}.tar.gz s3://${S3_BACKUP_BUCKET}/analytics/
          
          # Clean up
          rm -rf $ANALYTICS_DIR ${ANALYTICS_DIR}.tar.gz
          
          echo "Analytics backup completed: ${ANALYTICS_DIR}.tar.gz"

  send-summary:
    name: Send Backup Summary
    runs-on: ubuntu-latest
    needs: [backup-production, test-restore, backup-analytics]
    if: always()
    steps:
      - name: Send success notification
        if: needs.backup-production.result == 'success' && needs.test-restore.result == 'success'
        uses: 8398a7/action-slack@v3
        with:
          status: success
          webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
          text: |
            ‚úÖ **Daily Database Backup Completed Successfully**
            
            - Production database backed up
            - Restore process tested and verified
            - Analytics data exported
            - Old backups cleaned up
            
            All backup operations completed successfully.

      - name: Send partial failure notification
        if: needs.backup-production.result == 'success' && needs.test-restore.result != 'success'
        uses: 8398a7/action-slack@v3
        with:
          status: warning
          webhook_url: ${{ secrets.SLACK_WEBHOOK_URL }}
          text: |
            ‚ö†Ô∏è **Database Backup Completed with Warnings**
            
            - ‚úÖ Production database backed up
            - ‚ùå Restore test failed
            - ‚ÑπÔ∏è Analytics backup status: ${{ needs.backup-analytics.result }}
            
            **Action Required:** Investigate restore test failure.